"""
File   : chat_agent.py
Desc   : èŠå¤©æ™ºèƒ½ä½“
Date   : 2025/12/21
Author : Tianyu Chen
"""

import os
import time
import chainlit as cl
from openai import AsyncOpenAI
from loguru import logger

# å¼•å…¥ä¼˜åŒ–åçš„ UI å·¥å…·
from src.ui import get_thinking_html, get_finished_thinking_html


# åˆå§‹åŒ– Client
client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"), base_url=os.environ.get("OPENAI_BASE_URL"))


async def chat(message: cl.Message):
    """
    ä¸»èŠå¤©å…¥å£å‡½æ•°
    """
    logger.info("\n\n\n==================[System] New message received. Processing...==================")

    # 0. è·å–å†å²è®°å½•ä¸è®¾ç½®
    message_history = cl.user_session.get("message_history", [])
    model_settings = cl.user_session.get("model_settings")
    user_query = message.content
    
    logger.info(f"\n[User] {message.content}")

    # 1. UI æ¶ˆæ¯å®¹å™¨
    final_answer = cl.Message(content="")
    await final_answer.send()
    
    # 2. æ ¹æ®è®¾ç½®é€‰æ‹©å¤„ç†æ¨¡å¼
    answer_content = ""
    start_time = time.time()

    try:
        if model_settings["Streaming"]:
            logger.info("\n[System] Mode: Streaming")
            answer_content = await process_streaming_response(
                client, model_settings, message_history, user_query, final_answer, start_time
            )
        else:
            logger.info("\n[System] Mode: Blocking (Non-Streaming)")
            answer_content = await process_blocking_response(
                client, model_settings, message_history, user_query, final_answer, start_time
            )
    except Exception as e:
        error_msg = f"Error during generation: {str(e)}"
        logger.error(f"[System] {error_msg}")
        final_answer.content += f"\n\nâš ï¸ {error_msg}"
        await final_answer.update()
        return

    # 3. å°†çº¯å›ç­”æ–‡æœ¬å­˜å…¥å†å²è®°å¿†
    message_history.append({"role": "assistant", "content": answer_content})
    cl.user_session.set("message_history", message_history)

    logger.info("\n==================[System] Message processing completed.]==================\n\n")


async def call_model(client, model_settings, message_history, user_query):
    """
    è°ƒç”¨èŠå¤©æ¨¡å‹æ¥å£
    """

    PROMPT = f"""
    # è§’è‰²è®¾å®š
    {model_settings['RoleSetting']}

    # ç”¨æˆ·é—®é¢˜
    {user_query}
    """

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
    message_history.append({"role": "user", "content": PROMPT})

    response = await client.chat.completions.create(
        model=model_settings["Model"],
        messages=message_history,
        temperature=model_settings["Temperature"],
        max_tokens=int(model_settings["MaxTokens"]),
        stream=model_settings["Streaming"],
        extra_body={"enable_thinking": model_settings["Thinking"]}
    )
    return response

async def process_streaming_response(client, model_settings, message_history, user_query, final_answer, start_time):
    """
    å¤„ç†æµå¼è¾“å‡º (Streaming = True)
    """
    thinking_buffer = ""
    answer_content = ""
    is_thinking_phase = model_settings["Thinking"]

    stream = await call_model(client, model_settings, message_history, user_query)

     # === A. å¤„ç†æµå¼å“åº” ===
    async for chunk in stream:
        delta = chunk.choices[0].delta
        
        # å…¼å®¹ä¸åŒå‚å•†çš„ reasoning å­—æ®µ (DeepSeek é€šå¸¸ç”¨ reasoning_content)
        reasoning = getattr(delta, "reasoning_content", None)
        content = delta.content

        # === A. å¤„ç†æ€è€ƒ (Reasoning) ===
        if reasoning and is_thinking_phase:
            thinking_buffer += reasoning
            final_answer.content = get_thinking_html(thinking_buffer)
            await final_answer.update()
            # print(reasoning, end="", flush=True) # å¯é€‰ï¼šå‡å°‘æ§åˆ¶å°å™ªéŸ³
        
        # === B. å¤„ç†æ­£æ–‡ (Content) ===
        elif content:
            if is_thinking_phase:
                duration = int(time.time() - start_time)
                if duration < 1: duration = 1
                
                # ç»“æŸæ€è€ƒé˜¶æ®µï¼Œé”å®š HTML
                final_thinking_html = get_finished_thinking_html(thinking_buffer, duration)
                final_answer.content = final_thinking_html

                # æ·»åŠ ä¸¤ä¸ªæ¢è¡Œç¬¦ï¼Œå¼ºåˆ¶å°†åç»­å†…å®¹ä¸ HTML åˆ†ç¦»
                final_answer.content += "\n\n"
                
                is_thinking_phase = False 

                logger.debug(f"\n[ğŸ§  Thinking] {thinking_buffer}")
                logger.info(f"\n[System] Thinking finished. Duration: {duration}s")
            
            answer_content += content
            
            # å¿…é¡»é‡æ–°æ‹¼æ¥ï¼šå·²å®Œæˆçš„æ€è€ƒHTML + å½“å‰ç”Ÿæˆçš„æ­£æ–‡
            # æ³¨æ„ï¼šfinal_answer.content åœ¨ä¸Šé¢è¢«é‡ç½®ä¸º final_thinking_html äº†ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥ += å³å¯
            # ä½†ä¸ºäº†é˜²æ­¢é€»è¾‘æ··ä¹±ï¼Œå»ºè®®æ€»æ˜¯å…¨é‡èµ‹å€¼æˆ–ç¡®ä¿ content æ˜¯è¿½åŠ æ¨¡å¼
            # è¿™é‡Œç”±äº is_thinking_phase åˆ‡æ¢æ—¶å·²ç»é‡ç½®äº† contentï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥è¿½åŠ 
            final_answer.content += content 
            await final_answer.update()
            # print(content, end="", flush=True)

    # === C. å…œåº•å¤„ç† (å¦‚æœåªæœ‰æ€è€ƒæ²¡æœ‰å†…å®¹ï¼Œæˆ–è€…æµç»“æŸæ—¶è¿˜åœ¨æ€è€ƒ) ===
    if is_thinking_phase and thinking_buffer:
        duration = int(time.time() - start_time)
        final_answer.content = get_finished_thinking_html(thinking_buffer, duration)
        await final_answer.update()

    logger.debug(f"\n[ğŸ§¸ Answer] {answer_content}")
    logger.info("\n[System] Stream finished.")
    return answer_content


async def process_blocking_response(client, model_settings, message_history, user_query, final_answer, start_time):
    """
    å¤„ç†éæµå¼è¾“å‡º (Streaming = False)
    """
    # æç¤ºç”¨æˆ·æ­£åœ¨ç­‰å¾…å“åº”
    final_answer.content = "ç”Ÿæˆä¸­..."
    await final_answer.update()

    response = await call_model(client, model_settings, message_history, user_query)
    message = response.choices[0].message
    
    # è·å–å†…å®¹å’Œæ€è€ƒè¿‡ç¨‹
    answer_content = message.content if message.content else ""
    # éæµå¼æ¨¡å¼ä¸‹ï¼Œreasoning_content é€šå¸¸ä¹Ÿåœ¨ message å¯¹è±¡ä¸­
    reasoning_content = getattr(message, "reasoning_content", "")
    
    duration = int(time.time() - start_time)
    if duration < 1: duration = 1

    # æ„å»ºæœ€ç»ˆ UI å†…å®¹
    final_ui_content = ""

    # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œå…ˆæ·»åŠ æ€è€ƒå—
    if reasoning_content:
        final_ui_content += get_finished_thinking_html(reasoning_content, duration)
        # HTML å—ä¹‹åæ·»åŠ æ¢è¡Œ
        final_ui_content += "\n\n"
        logger.debug(f"\n[ğŸ§  Thinking] {reasoning_content}")

    # æ·»åŠ æ­£æ–‡
    final_ui_content += answer_content

    # ä¸€æ¬¡æ€§æ›´æ–° UI
    final_answer.content = final_ui_content
    await final_answer.update()

    logger.debug(f"\n[ğŸ§¸ Answer] {answer_content}")
    logger.info(f"\n[System] Request finished. Duration: {duration}s")
    return answer_content
